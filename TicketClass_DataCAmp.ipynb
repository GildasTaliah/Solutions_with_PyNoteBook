{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GildasTaliah/Solutions_with_PyNoteBook/blob/main/TicketClass_DataCAmp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "![servicedesk](servicedesk.png)\n",
        "\n",
        "CleverSupport is a company at the forefront of AI innovation, specializing in the development of AI-driven solutions to enhance customer support services. Their latest endeavor is to engineer a text classification system that can automatically categorize customer complaints.\n",
        "\n",
        "Your role as a data scientist involves the creation of a sophisticated machine learning model that can accurately assign complaints to specific categories, such as mortgage, credit card, money transfers, debt collection, etc."
      ],
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 165,
        "lastExecutedAt": 1707667023665,
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "CleverSupport is a company at the forefront of AI innovation, specializing in the development of AI-driven solutions to enhance customer support services. Their latest endeavor is to engineer a text classification system that can autonomously categorize customer complaints. \n\nYour role as a data scientist involves the creation of a sophisticated machine learning model that can accurately assign complaints to specific categories, such as technical issues, billing inquiries, cancellation requests, refunds, and product information requests.",
        "id": "e5870ae0-6165-459e-9c40-0f282883be7b"
      },
      "id": "e5870ae0-6165-459e-9c40-0f282883be7b",
      "cell_type": "markdown"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VOlzMT1o8sOR"
      },
      "id": "VOlzMT1o8sOR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "peoaXzA06g-e"
      },
      "id": "peoaXzA06g-e"
    },
    {
      "source": [
        "!pip install torchmetrics"
      ],
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 5319,
        "lastExecutedAt": 1712739671517,
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "!pip install torchmetrics",
        "outputsMetadata": {
          "0": {
            "height": 616,
            "type": "stream"
          }
        },
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true,
          "source_hidden": false
        },
        "lastExecutedByKernel": "9f04e6f3-6997-48a4-9918-8734a4d0ddb5",
        "id": "0dd4beb4-2329-4b0d-8a34-2354ee9c7fb4",
        "outputId": "4f7518dd-c6c0-421c-c436-1efeafd444c1"
      },
      "id": "0dd4beb4-2329-4b0d-8a34-2354ee9c7fb4",
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Defaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: torchmetrics in /home/repl/.local/lib/python3.8/site-packages (1.3.2)\nRequirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (1.23.2)\nRequirement already satisfied: packaging>17.1 in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (21.3)\nRequirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (1.13.0)\nRequirement already satisfied: lightning-utilities>=0.8.0 in /home/repl/.local/lib/python3.8/site-packages (from torchmetrics) (0.11.2)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (4.9.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (65.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>17.1->torchmetrics) (3.0.9)\nRequirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->torchmetrics) (11.7.99)\nRequirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->torchmetrics) (8.5.0.96)\nRequirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->torchmetrics) (11.10.3.66)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->torchmetrics) (11.7.99)\nRequirement already satisfied: wheel in /usr/local/lib/python3.8/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->torchmetrics) (0.38.4)\n"
        }
      ]
    },
    {
      "source": [
        "from collections import Counter\n",
        "import nltk, json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torchmetrics import Accuracy, Precision, Recall"
      ],
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "executionCancelledAt": null,
        "executionTime": 3243,
        "lastExecutedAt": 1712740302035,
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "from collections import Counter\nimport nltk\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom torchmetrics import Accuracy, Precision, Recall",
        "lastExecutedByKernel": "9f04e6f3-6997-48a4-9918-8734a4d0ddb5",
        "id": "2fa90b61-0244-4236-aa93-e33a7a088eec"
      },
      "id": "2fa90b61-0244-4236-aa93-e33a7a088eec",
      "cell_type": "code",
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 66,
        "lastExecutedAt": 1712740302102,
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "nltk.download('punkt')",
        "outputsMetadata": {
          "0": {
            "height": 80,
            "type": "stream"
          }
        },
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true,
          "source_hidden": false
        },
        "lastExecutedByKernel": "9f04e6f3-6997-48a4-9918-8734a4d0ddb5",
        "id": "37a51a81-1301-4a80-b8c6-716faaff4c5c",
        "outputId": "a636c05d-7453-45ad-bf37-fd01c3653e44"
      },
      "id": "37a51a81-1301-4a80-b8c6-716faaff4c5c",
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "[nltk_data] Downloading package punkt to /home/repl/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "True"
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "source": [
        "# Import data and labels\n",
        "with open(\"words.json\", 'r') as f1:\n",
        "    words = json.load(f1)\n",
        "with open(\"text.json\", 'r') as f2:\n",
        "    text = json.load(f2)\n",
        "labels = np.load('labels.npy')"
      ],
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 168,
        "lastExecutedAt": 1712740315283,
        "lastExecutedByKernel": "9f04e6f3-6997-48a4-9918-8734a4d0ddb5",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "import json\nlabels = np.load('labels.npy')\nwith open(\"words.json\", 'r') as f1:\n    words = json.load(f1)\nwith open(\"text.json\", 'r') as f2:\n    text = json.load(f2)",
        "id": "e1b12eaf-e55c-422c-94a2-b0197c465a1b"
      },
      "id": "e1b12eaf-e55c-422c-94a2-b0197c465a1b",
      "cell_type": "code",
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Dictionaries to store the word to index mappings and vice versa\n",
        "word2idx = {o:i for i,o in enumerate(words)}\n",
        "idx2word = {i:o for i,o in enumerate(words)}\n",
        "\n",
        "# Looking up the mapping dictionary and assigning the index to the respective words\n",
        "for i, sentence in enumerate(text):\n",
        "    text[i] = [word2idx[word] if word in word2idx else 0 for word in sentence]\n",
        "\n",
        "# Defining a function that either shortens sentences or pads sentences with 0 to a fixed length\n",
        "def pad_input(sentences, seq_len):\n",
        "    features = np.zeros((len(sentences), seq_len),dtype=int)\n",
        "    for ii, review in enumerate(sentences):\n",
        "        if len(review) != 0:\n",
        "            features[ii, -len(review):] = np.array(review)[:seq_len]\n",
        "    return features\n",
        "\n",
        "text = pad_input(text, 50)"
      ],
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 183,
        "lastExecutedAt": 1712740317656,
        "lastExecutedByKernel": "9f04e6f3-6997-48a4-9918-8734a4d0ddb5",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "# Dictionaries to store the word to index mappings and vice versa\nword2idx = {o:i for i,o in enumerate(words)}\nidx2word = {i:o for i,o in enumerate(words)}\n\nfor i, sentence in enumerate(text):\n    # Looking up the mapping dictionary and assigning the index to the respective words\n    text[i] = [word2idx[word] if word in word2idx else 0 for word in sentence]",
        "id": "d630badb-23dd-4368-9a96-e2b478ad5cff"
      },
      "id": "d630badb-23dd-4368-9a96-e2b478ad5cff",
      "cell_type": "code",
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Splitting dataset\n",
        "train_text, test_text, train_label, test_label = train_test_split(text, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "train_data = TensorDataset(torch.from_numpy(train_text), torch.from_numpy(train_label).long())\n",
        "test_data = TensorDataset(torch.from_numpy(test_text), torch.from_numpy(test_label).long())"
      ],
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 11,
        "lastExecutedAt": 1712740330447,
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "train_text, test_text, train_label, test_label = train_test_split(text, labels, test_size=0.2, random_state=42)\n\ntrain_data = TensorDataset(torch.from_numpy(train_text), torch.from_numpy(train_label).long())\ntest_data = TensorDataset(torch.from_numpy(test_text), torch.from_numpy(test_label).long())",
        "lastExecutedByKernel": "9f04e6f3-6997-48a4-9918-8734a4d0ddb5",
        "id": "f2654836-631f-415e-9922-5ab3bafaaafa"
      },
      "id": "f2654836-631f-415e-9922-5ab3bafaaafa",
      "cell_type": "code",
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Start coding here"
      ],
      "metadata": {
        "executionCancelledAt": null,
        "executionTime": 12,
        "lastExecutedAt": 1712737435231,
        "lastExecutedByKernel": "9f04e6f3-6997-48a4-9918-8734a4d0ddb5",
        "lastScheduledRunId": null,
        "lastSuccessfullyExecutedCode": "labels = np.load('labels.npy')",
        "id": "d2b3c50c-66b1-40ed-a17c-038e7addc7ec"
      },
      "id": "d2b3c50c-66b1-40ed-a17c-038e7addc7ec",
      "cell_type": "code",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://projects.datacamp.com/projects/2148\n",
        "\n",
        "To be continued"
      ],
      "metadata": {
        "id": "rNb_hDku6h2K"
      },
      "id": "rNb_hDku6h2K"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}